<!DOCTYPE html>
<html lang="ar">
<head>
  <meta charset="UTF-8" />
  <title>تتبع اليد والوجه مع نبض القلب وتعبير الوجه</title>
  <style>
    html, body {
      margin: 0; padding: 0; overflow: hidden; background-color: black; color: white;
      font-family: Arial, sans-serif;
    }
    video, canvas {
      position: absolute; top: 0; left: 0; width: 100vw; height: 100vh; object-fit: cover;
    }
    #info {
      position: absolute; top: 10px; left: 10px; z-index: 10;
      background: rgba(0,0,0,0.5); padding: 10px; border-radius: 8px;
      max-width: 300px;
    }
  </style>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="canvas"></canvas>
  <div id="info">
    <div>نبضات القلب: <span id="bpm">--</span> نبضة/دقيقة</div>
    <div>الحالة العاطفية: <span id="emotion">جاري التحليل...</span></div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/holistic/holistic.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const bpmElement = document.getElementById('bpm');
    const emotionElement = document.getElementById('emotion');

    function resizeCanvas() {
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
    }

    const holistic = new Holistic({
      locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/holistic/${file}`
    });

    holistic.setOptions({
      modelComplexity: 1,
      smoothLandmarks: true,
      enableSegmentation: false,
      refineFaceLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5
    });

    // مخزن لقيم اللون الأخضر (من الجبهة) لتحليل نبضات القلب
    const greenSamples = [];
    const sampleInterval = 33; // تقريبا 30 FPS
    const maxSamples = 300;    // تخزين 10 ثواني (300 * 33ms = ~10s)

    // دالة لحساب المتوسط من مجموعة أرقام
    function mean(arr) {
      return arr.reduce((a,b) => a + b, 0) / arr.length;
    }

    // دالة لحساب تردد تقريبي باستخدام تحليل بسيط (zero-crossing count)
    function estimateHeartRate(samples, fps) {
      let meanVal = mean(samples);
      let zeroCrossings = 0;
      for(let i=1; i < samples.length; i++) {
        if ((samples[i-1] - meanVal) * (samples[i] - meanVal) < 0) {
          zeroCrossings++;
        }
      }
      const freq = zeroCrossings / (2 * (samples.length / fps)); // تردد بالهرتز
      const bpm = Math.round(freq * 60);
      return bpm;
    }

    // دالة تحليل تعبير الوجه البسيط
    function analyzeExpression(faceLandmarks) {
      // نقاط الفم: 13 = أسفل الشفة العليا, 14 = أعلى الشفة السفلى
      // نقاط العين اليمنى: 159 (الجفن العلوي), 145 (الجفن السفلي)
      // نقاط العين اليسرى: 386 (الجفن العلوي), 374 (الجفن السفلي)

      const mouthOpen = distance(faceLandmarks[13], faceLandmarks[14]) > 0.04; // عتبة مفتوحة
      const rightEyeOpen = distance(faceLandmarks[159], faceLandmarks[145]) > 0.015;
      const leftEyeOpen = distance(faceLandmarks[386], faceLandmarks[374]) > 0.015;

      if (!mouthOpen && !rightEyeOpen && !leftEyeOpen) return "نائم";
      if (mouthOpen && rightEyeOpen && leftEyeOpen) return "جوعان أو يتكلم";
      if (!mouthOpen && rightEyeOpen && leftEyeOpen) return "منتبه";
      return "غير محدد";
    }

    // دالة حساب المسافة بين نقطتين (نسبة على الوجه)
    function distance(a, b) {
      const dx = a.x - b.x;
      const dy = a.y - b.y;
      return Math.sqrt(dx*dx + dy*dy);
    }

    holistic.onResults(results => {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.drawImage(results.image, 0, 0, canvas.width, canvas.height);

      // رسم ملامح الوجه واليدين
      if (results.faceLandmarks) {
        drawConnectors(ctx, results.faceLandmarks, FACEMESH_TESSELATION, { color: 'rgba(0,255,0,0.3)', lineWidth: 1 });
        drawLandmarks(ctx, results.faceLandmarks, { color: 'rgba(255,0,0,0.6)', radius: 1.5 });
      }
      if (results.leftHandLandmarks) {
        drawConnectors(ctx, results.leftHandLandmarks, HAND_CONNECTIONS, { color: 'rgba(0,255,0,0.8)', lineWidth: 3 });
        drawLandmarks(ctx, results.leftHandLandmarks, { color: 'rgba(255,0,0,0.8)', radius: 4 });
      }
      if (results.rightHandLandmarks) {
        drawConnectors(ctx, results.rightHandLandmarks, HAND_CONNECTIONS, { color: 'rgba(0,255,0,0.8)', lineWidth: 3 });
        drawLandmarks(ctx, results.rightHandLandmarks, { color: 'rgba(255,0,0,0.8)', radius: 4 });
      }

      // قياس اللون الأخضر في الجبهة (نقاط محددة)
      if (results.faceLandmarks) {
        // نقاط الجبهة (اختيار بعض النقاط بين 10 و 338)
        const foreheadIndices = [10, 338, 297, 332, 284, 251];
        let greenSum = 0;
        let count = 0;

        foreheadIndices.forEach(idx => {
          const x = results.faceLandmarks[idx].x * canvas.width;
          const y = results.faceLandmarks[idx].y * canvas.height;

          // نقرأ لون البكسل من الصورة في الكانفس
          const pixel = ctx.getImageData(x, y, 1, 1).data;
          greenSum += pixel[1]; // اللون الأخضر
          count++;
        });

        const avgGreen = greenSum / count;
        greenSamples.push(avgGreen);

        // المحافظة على حجم المخزن
        if (greenSamples.length > maxSamples) {
          greenSamples.shift();
        }

        // تقدير نبضات القلب كل 10 ثواني تقريبًا
        if (greenSamples.length === maxSamples) {
          const bpm = estimateHeartRate(greenSamples, 30); // fps = 30
          if (bpm > 40 && bpm < 180) {
            bpmElement.textContent = bpm;
          } else {
            bpmElement.textContent = '--';
          }
        }
      }

      // تحليل التعبير إذا كانت ملامح الوجه موجودة
      if (results.faceLandmarks) {
        const emotion = analyzeExpression(results.faceLandmarks);
        emotionElement.textContent = emotion;
      } else {
        emotionElement.textContent = "لا يوجد وجه";
        bpmElement.textContent = '--';
      }
    });

    async function start() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;

      video.onloadedmetadata = () => {
        video.play();
        resizeCanvas();
        requestAnimationFrame(loop);
      };
    }

    async function loop() {
      if (video.readyState === 4) {
        await holistic.send({ image: video });
      }
      requestAnimationFrame(loop);
    }

    start();
  </script>
</body>
</html>